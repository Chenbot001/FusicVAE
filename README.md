# FusicVAE
Traditionally, models like Recurrent Neural Networks (RNNs) and Transformers have been at the forefront of audio learning, leveraging their sequential data processing capabilities to capture the temporal dynamics of music. High-fidelity music generation models, such as OpenAI’s Jukebox or
Google DeepMind’s WaveNet, have set benchmarks in generating music in various styles, demonstrating the potential of deep learning in creating complex audio sequences. However, these models often require substantial computational resources and extended training times, posing significant barriers for smaller projects like this one. Instead, we could approach the task from an image generation perspective.

This study adopts a Convolutional Variational Autoencoder (VAE) approach to music generation, informed by the visual analogy of mel-spectrograms in audio processing, where audio signals are transformed into visual representations that capture temporal and frequency information in a fashion similar to images. Compared to conventional spectrograms, mel specs has its frequency scale (y-axis) mapped on a logarithmic scale, which represents audio data in a form more consistent with how the human ear perceives audio frequencies.

The decision to pivot from conventional high-quality methods such as RNNs and Transformers to a convolutional VAE was driven by practical considerations of computational resources and time constraints. Despite the proven efficacy of these advanced models in generating nuanced and diverse musical compositions, their computational demands exceed the scope of this project’s available resources. Instead, by treating music generation as an image-processing task, the project leverages the convolutional VAE’s efficiency in capturing the intricate patterns within the spectrograms of the target music style.
